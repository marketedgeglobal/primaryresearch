name: Weekly Sheet Analysis

on:
  workflow_dispatch:
  # schedule:
  #   - cron: '0 9 * * 1'

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write
    env:
      SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
      AI_API_KEY: ${{ secrets.AI_API_KEY }}
      SERVICE_ACCOUNT_PATH: /tmp/gservice.json
      SHEET_RANGE: Sheet1!A1:Z
      AI_PROVIDER: openai
      AI_MODEL: gpt-4o-mini
      OUTPUT_DIR: analyses
      ALLOW_MOCK: 'true'
      AI_TIMEOUT_SECONDS: '60'
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      EMAIL_ENABLED: ${{ secrets.EMAIL_ENABLED }}
      EMAIL_SMTP_HOST: ${{ secrets.EMAIL_SMTP_HOST }}
      EMAIL_SMTP_PORT: ${{ secrets.EMAIL_SMTP_PORT }}
      EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      EMAIL_TO: ${{ secrets.EMAIL_TO }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Write service account key
        env:
          GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}
        run: |
          test -n "$GOOGLE_SERVICE_ACCOUNT_KEY"
          printf '%s' "$GOOGLE_SERVICE_ACCOUNT_KEY" > /tmp/gservice.json

      - name: Generate run metadata
        id: runmeta
        run: |
          RUN_ID=$(python - <<'PY'
          import os
          import sys
          sys.path.append("scripts")
          from run_metadata import generate_run_metadata, save_run_metadata

          metadata = generate_run_metadata()
          save_run_metadata("run_metadata.json", metadata)
          print(metadata["run_id"])
          PY
          )
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"

      - name: Fetch sheet rows
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          python scripts/fetch_sheet.py \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Call AI provider
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          # TODO: Replace mock/placeholder logic in scripts/call_ai.py with your provider endpoint/model.
          # TODO: Keep provider credentials in GitHub Secrets only.
          python scripts/call_ai.py \
            --input "$OUTPUT_DIR/rows-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Validate analysis schema
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          python scripts/validate_schema.py \
            --schema schemas/analysis_schema.json \
            --input "$OUTPUT_DIR/analysis-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Save weekly analysis
        run: |
          mkdir -p analyses
          cp "$OUTPUT_DIR/analysis-${{ steps.runmeta.outputs.run_id }}.json" "analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json"

      - name: Update dashboard files in docs
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          set -euo pipefail
          mkdir -p docs

          SUMMARY_FILE="$OUTPUT_DIR/summary-$RUN_ID.md"
          if [ ! -f "$SUMMARY_FILE" ]; then
            echo "Expected summary file not found: $SUMMARY_FILE"
            exit 1
          fi

          cp "$SUMMARY_FILE" "docs/summary-$RUN_ID.md"
          printf '<meta http-equiv="refresh" content="0; url=summary-%s.md" />\n' "$RUN_ID" > docs/latest.html

          python - <<'PY'
          from pathlib import Path
          import os

          run_id = os.environ["RUN_ID"]
          history_path = Path("docs/history.md")
          entry = f"- [summary-{run_id}.md](summary-{run_id}.md)"

          if history_path.exists():
            raw_lines = history_path.read_text(encoding="utf-8").splitlines()
          else:
            raw_lines = []

          body_lines = []
          for line in raw_lines:
            stripped = line.strip()
            if stripped == "# Historical Reports":
              continue
            if stripped == entry:
              continue
            if not stripped and (not body_lines or not body_lines[-1].strip()):
              continue
            body_lines.append(line.rstrip())

          output_lines = ["# Historical Reports", "", entry]
          if body_lines:
            output_lines.append("")
            output_lines.extend(body_lines)

          history_path.write_text("\n".join(output_lines).rstrip() + "\n", encoding="utf-8")
          PY

          HISTORICAL_COUNT=$(find analyses -maxdepth 1 -name 'weekly-*.json' | wc -l | tr -d '[:space:]')
          if [ "${HISTORICAL_COUNT}" -gt 0 ]; then
            python scripts/charting.py --analyses-dir analyses --charts-dir docs/charts
          else
            echo "No historical analysis files found; skipping chart generation."
          fi

          python scripts/dashboard_renderer.py \
            --template docs/dashboard_template.md \
            --analyses-dir analyses \
            --docs-dir docs \
            --metadata run_metadata.json \
            --output docs/index.md

      - name: Upload weekly analysis artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weekly-analysis-${{ steps.runmeta.outputs.run_id }}
          path: |
            ${{ env.OUTPUT_DIR }}/analysis-${{ steps.runmeta.outputs.run_id }}.json
            ${{ env.OUTPUT_DIR }}/summary-${{ steps.runmeta.outputs.run_id }}.md
            analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json
          if-no-files-found: warn

      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Commit weekly analysis and dashboard
        run: |
          set -euo pipefail
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add run_metadata.json "analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json" docs/
          if git diff --cached --quiet; then
            echo 'No changes to commit.'
            exit 0
          fi
          git commit -m "chore: update weekly analysis and dashboard ${{ steps.runmeta.outputs.run_id }}"
          git push

      - name: Upload analysis artifact on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-failure-artifacts
          path: |
            analyses/analysis-*.json
            analyses/rows-*.json
            analyses/error-*.json
            pipeline_error.json
          if-no-files-found: ignore

# Suggested commit message: feat: add richer GitHub Pages dashboard layout with auto-generated sections and optional charts
