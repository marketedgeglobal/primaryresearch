name: Weekly Sheet Analysis

on:
  workflow_dispatch:
  schedule:
    - cron: '0 9 * * 1'

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write
    env:
      SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
      AI_API_KEY: ${{ secrets.AI_API_KEY }}
      SERVICE_ACCOUNT_PATH: /tmp/gservice.json
      SHEET_RANGE: Sheet1!A1:Z
      AI_PROVIDER: openai
      AI_MODEL: gpt-4o-mini
      FOLLOWUP_MODEL: gpt-4.1-mini
      OUTPUT_DIR: analyses
      ALLOW_MOCK: 'true'
      AI_TIMEOUT_SECONDS: '60'
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      EMAIL_ENABLED: ${{ secrets.EMAIL_ENABLED }}
      EMAIL_SMTP_HOST: ${{ secrets.EMAIL_SMTP_HOST }}
      EMAIL_SMTP_PORT: ${{ secrets.EMAIL_SMTP_PORT }}
      EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      EMAIL_TO: ${{ secrets.EMAIL_TO }}
      CHAIN_MAX_DEPTH: '2'
      CHAIN_MAX_BRANCHES: '2'
      CHAIN_TIMEOUT_SEC: '45'
      CHAIN_MIN_CONFIDENCE_DELTA: '0.08'
      CHAIN_BUDGET_USD: '0.50'
      CHAIN_WORKFLOW_TIMEOUT_SEC: '180'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Validate required configuration
        run: |
          set -euo pipefail

          missing=0
          if [ -z "${SPREADSHEET_ID:-}" ]; then
            echo "Missing required secret: SPREADSHEET_ID"
            missing=1
          fi
          if [ -z "${AI_API_KEY:-}" ]; then
            echo "Missing required secret: AI_API_KEY"
            missing=1
          fi
          if [ -z "${GOOGLE_SERVICE_ACCOUNT_KEY:-}" ]; then
            echo "Missing required secret: GOOGLE_SERVICE_ACCOUNT_KEY"
            missing=1
          fi

          if [ "$missing" -ne 0 ]; then
            echo "Add the missing secret(s) under Settings -> Secrets and variables -> Actions, then re-run this workflow."
            exit 1
          fi

          echo "Required configuration is present."
        env:
          GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}

      - name: Write service account key
        env:
          GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}
        run: |
          set +x
          test -n "$GOOGLE_SERVICE_ACCOUNT_KEY"
          printf '%s' "$GOOGLE_SERVICE_ACCOUNT_KEY" > /tmp/gservice.json

      - name: Generate run metadata
        id: runmeta
        run: |
          set -euo pipefail
          RUN_ID=$(python - <<'PY'
          import os
          import sys
          sys.path.append("scripts")
          from run_metadata import generate_run_metadata, save_run_metadata

          metadata = generate_run_metadata()
          save_run_metadata("run_metadata.json", metadata)
          print(metadata["run_id"])
          PY
          )
          RUN_ID="$(printf '%s' "$RUN_ID" | tr -d '\r\n')"
          if [ -z "$RUN_ID" ]; then
            echo "Generated RUN_ID is empty"
            exit 1
          fi
          echo "RUN_ID=$RUN_ID" >> "$GITHUB_ENV"
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"

      - name: Fetch sheet rows
        run: |
          python scripts/fetch_sheet.py \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Call AI provider
        run: |
          # TODO: Replace mock/placeholder logic in scripts/call_ai.py with your provider endpoint/model.
          # TODO: Keep provider credentials in GitHub Secrets only.
          python scripts/call_ai.py \
            --input "$OUTPUT_DIR/rows-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Validate analysis schema
        run: |
          python scripts/validate_schema.py \
            --schema schemas/analysis_schema.json \
            --input "$OUTPUT_DIR/analysis-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Save weekly analysis
        run: |
          mkdir -p analyses
          cp "$OUTPUT_DIR/analysis-$RUN_ID.json" "analyses/weekly-$RUN_ID.json"

      - name: Update dashboard files in docs
        run: |
          set -euo pipefail
          mkdir -p docs docs/assets docs/partners

          SUMMARY_FILE="$OUTPUT_DIR/summary-$RUN_ID.md"
          if [ ! -f "$SUMMARY_FILE" ]; then
            echo "Expected summary file not found: $SUMMARY_FILE"
            exit 1
          fi

          cp "$SUMMARY_FILE" "docs/summary-$RUN_ID.md"
          printf '<meta http-equiv="refresh" content="0; url=summary-%s.md" />\n' "$RUN_ID" > docs/latest.html

          python - <<'PY'
          from pathlib import Path
          import os

          run_id = os.environ["RUN_ID"]
          history_path = Path("docs/history.md")
          entry = f"- [summary-{run_id}.md](summary-{run_id}.md)"

          if history_path.exists():
            raw_lines = history_path.read_text(encoding="utf-8").splitlines()
          else:
            raw_lines = []

          body_lines = []
          for line in raw_lines:
            stripped = line.strip()
            if stripped == "# Historical Reports":
              continue
            if stripped == entry:
              continue
            if not stripped and (not body_lines or not body_lines[-1].strip()):
              continue
            body_lines.append(line.rstrip())

          output_lines = ["# Historical Reports", "", entry]
          if body_lines:
            output_lines.append("")
            output_lines.extend(body_lines)

          history_path.write_text("\n".join(output_lines).rstrip() + "\n", encoding="utf-8")
          PY

          HISTORICAL_COUNT=$(find analyses -maxdepth 1 \( -name 'weekly-*.json' -o -name 'analysis-*.json' \) | wc -l | tr -d '[:space:]')
          if [ "${HISTORICAL_COUNT}" -gt 0 ]; then
            python scripts/trend_analysis.py \
              --analyses-dir analyses \
              --output docs/trend_data.json

            python scripts/theme_renderer.py \
              --analyses-dir analyses \
              --template docs/theme_template.md \
              --themes-dir docs/themes \
              --charts-dir docs/charts/themes

            python scripts/comparative_analysis.py \
              --analyses-dir analyses \
              --output docs/comparative_data.json

            python scripts/charting.py \
              --analyses-dir analyses \
              --charts-dir docs/charts \
              --trend-data docs/trend_data.json \
              --comparative-data docs/comparative_data.json \
              --comparative-charts-dir docs/charts/comparative \
              --comparative-markdown-output docs/comparative_charts.md

            python scripts/comparative_analysis.py \
              --analyses-dir analyses \
              --output docs/comparative_data.json \
              --markdown-output docs/comparative.md \
              --charts-markdown docs/comparative_charts.md \
              --docs-dir docs

            python scripts/insights.py \
              --run-id "$RUN_ID" \
              --analyses-dir analyses \
              --comparative-data docs/comparative_data.json \
              --trend-data docs/trend_data.json \
              --output-dir "$OUTPUT_DIR" \
              --analysis-json "$OUTPUT_DIR/analysis-$RUN_ID.json" \
              --weekly-analysis-json "analyses/weekly-$RUN_ID.json" \
              --summary-path "$OUTPUT_DIR/summary-$RUN_ID.md" \
              --docs-summary-path "docs/summary-$RUN_ID.md"

            python - <<'PY'
            import json
            import os
            import sys
            from pathlib import Path

            sys.path.append("scripts")
            from alerts import generate_alerts, load_playbooks, load_recent_analysis_history, write_alerts_output
            from config import load_config
            from log_utils import log

            run_id = os.environ["RUN_ID"]
            output_dir = os.environ.get("OUTPUT_DIR", "analyses")

            insights_path = Path(output_dir) / f"insights-{run_id}.json"
            if not insights_path.exists():
                raise FileNotFoundError(f"Missing insights file: {insights_path}")

            insights_payload = json.loads(insights_path.read_text(encoding="utf-8"))
            insights = insights_payload.get("insights") if isinstance(insights_payload, dict) else []
            if not isinstance(insights, list):
                insights = []

            playbooks = load_playbooks("scripts/playbooks/remediation_playbooks.yml")
            pipeline_cfg = load_config({"run_id": run_id, "output_dir": output_dir})
            pipeline_cfg["analysis_history"] = load_recent_analysis_history("analyses", run_id, max_runs=3)

            alerts = generate_alerts(insights, playbooks, pipeline_cfg)
            paths = write_alerts_output(run_id, alerts, output_dir)
            log(f"Generated {len(alerts)} automated alerts")
            high_with_followups = [
              item for item in alerts
              if isinstance(item, dict)
              and str(item.get("severity")) == "high"
              and str(item.get("followup_path") or "").strip()
            ]
            log(f"Generated {len(high_with_followups)} follow-up investigations for high-severity alerts")
            log(f"Alert outputs: {paths}")
            PY

            timeout "${CHAIN_WORKFLOW_TIMEOUT_SEC}s" python scripts/followups.py \
              --run-id "$RUN_ID" \
              --alerts "$OUTPUT_DIR/alerts-$RUN_ID.json" \
              --output-dir "$OUTPUT_DIR" \
              --templates "scripts/templates/chain_templates.yml"
          else
            echo "No historical analysis files found; skipping chart generation."
          fi

          python scripts/dashboard_renderer.py \
            --template docs/dashboard_template.md \
            --partner-template docs/partner_template.md \
            --analyses-dir analyses \
            --docs-dir docs \
            --metadata run_metadata.json \
            --trend-data docs/trend_data.json \
            --partners-dir docs/partners \
            --output docs/index.md

      - name: Upload weekly analysis artifact
        if: always() && env.RUN_ID != ''
        uses: actions/upload-artifact@v4
        with:
          name: weekly-analysis-${{ env.RUN_ID }}
          path: |
            ${{ env.OUTPUT_DIR }}/analysis-${{ env.RUN_ID }}.json
            ${{ env.OUTPUT_DIR }}/summary-${{ env.RUN_ID }}.md
            ${{ env.OUTPUT_DIR }}/insights-${{ env.RUN_ID }}.json
            ${{ env.OUTPUT_DIR }}/alerts-${{ env.RUN_ID }}.json
            ${{ env.OUTPUT_DIR }}/followup-${{ env.RUN_ID }}-*.json
            ${{ env.OUTPUT_DIR }}/followup-${{ env.RUN_ID }}-*-chain.json
            docs/insights-${{ env.RUN_ID }}.md
            docs/alerts-${{ env.RUN_ID }}.md
            docs/followup-${{ env.RUN_ID }}-*.md
            docs/followup-${{ env.RUN_ID }}-*-chain.md
            analyses/weekly-${{ env.RUN_ID }}.json
          if-no-files-found: warn

      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Commit weekly analysis and dashboard
        run: |
          set -euo pipefail
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add run_metadata.json "analyses/weekly-$RUN_ID.json" docs/
          if git diff --cached --quiet; then
            echo 'No changes to commit.'
            exit 0
          fi
          git commit -m "chore: update weekly analysis and dashboard $RUN_ID"
          git push

      - name: Upload analysis artifact on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-failure-artifacts
          path: |
            analyses/analysis-*.json
            analyses/rows-*.json
            analyses/error-*.json
            pipeline_error.json
          if-no-files-found: ignore

# Suggested commit message: feat: add agent-driven follow-ups for high-severity alerts
