name: Weekly Sheet Analysis

on:
  workflow_dispatch:
  # schedule:
  #   - cron: '0 9 * * 1'

jobs:
  analyze:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pages: write
      id-token: write
    env:
      SPREADSHEET_ID: ${{ secrets.SPREADSHEET_ID }}
      AI_API_KEY: ${{ secrets.AI_API_KEY }}
      SERVICE_ACCOUNT_PATH: /tmp/gservice.json
      SHEET_RANGE: Sheet1!A1:Z
      AI_PROVIDER: openai
      AI_MODEL: gpt-4o-mini
      OUTPUT_DIR: analyses
      ALLOW_MOCK: 'true'
      AI_TIMEOUT_SECONDS: '60'
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      EMAIL_ENABLED: ${{ secrets.EMAIL_ENABLED }}
      EMAIL_SMTP_HOST: ${{ secrets.EMAIL_SMTP_HOST }}
      EMAIL_SMTP_PORT: ${{ secrets.EMAIL_SMTP_PORT }}
      EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      EMAIL_TO: ${{ secrets.EMAIL_TO }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Write service account key
        env:
          GOOGLE_SERVICE_ACCOUNT_KEY: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_KEY }}
        run: |
          test -n "$GOOGLE_SERVICE_ACCOUNT_KEY"
          printf '%s' "$GOOGLE_SERVICE_ACCOUNT_KEY" > /tmp/gservice.json

      - name: Generate run metadata
        id: runmeta
        run: |
          RUN_ID=$(python - <<'PY'
          import os
          import sys
          sys.path.append("scripts")
          from run_metadata import generate_run_metadata, save_run_metadata

          metadata = generate_run_metadata()
          save_run_metadata("run_metadata.json", metadata)
          print(metadata["run_id"])
          PY
          )
          echo "run_id=$RUN_ID" >> "$GITHUB_OUTPUT"

      - name: Fetch sheet rows
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          python scripts/fetch_sheet.py \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Call AI provider
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          # TODO: Replace mock/placeholder logic in scripts/call_ai.py with your provider endpoint/model.
          # TODO: Keep provider credentials in GitHub Secrets only.
          python scripts/call_ai.py \
            --input "$OUTPUT_DIR/rows-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Validate analysis schema
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          python scripts/validate_schema.py \
            --schema schemas/analysis_schema.json \
            --input "$OUTPUT_DIR/analysis-$RUN_ID.json" \
            --run-id "$RUN_ID" \
            --output-dir "$OUTPUT_DIR"

      - name: Save weekly analysis
        run: |
          mkdir -p analyses
          cp "$OUTPUT_DIR/analysis-${{ steps.runmeta.outputs.run_id }}.json" "analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json"

      - name: Update dashboard files in docs
        env:
          RUN_ID: ${{ steps.runmeta.outputs.run_id }}
        run: |
          set -euo pipefail
          mkdir -p docs

          SUMMARY_FILE="$OUTPUT_DIR/summary-$RUN_ID.md"
          if [ ! -f "$SUMMARY_FILE" ]; then
            echo "Expected summary file not found: $SUMMARY_FILE"
            exit 1
          fi

          cp "$SUMMARY_FILE" docs/latest.md
          cp "$SUMMARY_FILE" "docs/summary-$RUN_ID.md"

          export TIMESTAMP_UTC="$(date -u +'%Y-%m-%d %H:%M:%S UTC')"

          python - <<'PY'
          from pathlib import Path
          import os

          index_path = Path("docs/index.md")
          run_id = os.environ["RUN_ID"]
          timestamp = os.environ["TIMESTAMP_UTC"]
          history_line = f"- [{timestamp}](summary-{run_id}.md)"

          content = index_path.read_text(encoding="utf-8")
          lines = content.splitlines()

          replaced = False
          for idx, line in enumerate(lines):
            if line.startswith("Last updated:"):
              lines[idx] = f"Last updated: {timestamp}"
              replaced = True
              break
          if not replaced:
            lines.insert(1, f"Last updated: {timestamp}")

          if history_line not in lines:
            try:
              header_idx = lines.index("## Historical Reports")
            except ValueError:
              lines.extend(["", "## Historical Reports", history_line])
            else:
              insert_idx = header_idx + 1
              while insert_idx < len(lines) and lines[insert_idx].strip() == "":
                insert_idx += 1
              lines.insert(insert_idx, history_line)

          index_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
          PY

      - name: Upload weekly analysis artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weekly-analysis-${{ steps.runmeta.outputs.run_id }}
          path: |
            ${{ env.OUTPUT_DIR }}/analysis-${{ steps.runmeta.outputs.run_id }}.json
            ${{ env.OUTPUT_DIR }}/summary-${{ steps.runmeta.outputs.run_id }}.md
            analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json
          if-no-files-found: warn

      - name: Configure GitHub Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Commit weekly analysis and dashboard
        run: |
          set -euo pipefail
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add run_metadata.json "analyses/weekly-${{ steps.runmeta.outputs.run_id }}.json" docs/
          if git diff --cached --quiet; then
            echo 'No changes to commit.'
            exit 0
          fi
          git commit -m "chore: update weekly analysis and dashboard ${{ steps.runmeta.outputs.run_id }}"
          git push

      - name: Upload analysis artifact on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-failure-artifacts
          path: |
            analyses/analysis-*.json
            analyses/rows-*.json
            analyses/error-*.json
            pipeline_error.json
          if-no-files-found: ignore

# Suggested commit message: feat: add GitHub Pages dashboard index and auto-update workflow
